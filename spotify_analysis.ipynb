{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1005b89",
   "metadata": {},
   "source": [
    "# STAT 315 Final Project  \n",
    "## Spotify Track Popularity Analysis  \n",
    "**Group Members:** Ayushi Sharma, Bernardo Gonzalez Guerra, Mohnish Bandari  \n",
    "**Course:** STAT 315 - Fall 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8651a6",
   "metadata": {},
   "source": [
    "# 1. Research Questions\n",
    "\n",
    "Our analysis focuses on the following questions:\n",
    "\n",
    "1. What audio features are the strongest predictors of a song being popular on Spotify?\n",
    "2. Can we accurately classify whether a song falls into a “high popularity” vs. “low popularity” group based solely on its audio features?\n",
    "3. How well can we predict a song’s exact popularity score using a regression model, and which features contribute most to this prediction?\n",
    "4. What correlations and relationships exist among audio features, and how might these influence model performance?\n",
    "5. Do certain genres or artists tend to have higher average popularity, and how do their audio profiles compare to less popular tracks?\n",
    "6. How much uncertainty exists in our predictions, as measured by cross-validation and bootstrap confidence intervals?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b08f0",
   "metadata": {},
   "source": [
    "# 2. Data Collection\n",
    "\n",
    "## Our dataset\n",
    "For this project, we are using the Spotify Tracks Dataset from Kaggle, which contains over 230,000 songs pulled from Spotify’s Web API. Each row represents a single track, and the dataset includes a mixture of audio features, metadata, and Spotify’s own popularity score. The audio features (such as danceability, energy, and acousticness) are numerical values generated by Spotify’s signal processing models that describe different musical characteristics of each track. Because the dataset is large, mostly numeric, and relatively clean, it is well suited for exploratory data analysis and predictive modeling.\n",
    "\n",
    "## Dataset Source\n",
    "This dataset is publicly available on Kaggle at:\n",
    "https://www.kaggle.com/datasets/zaheenhamidani/ultimate-spotify-tracks-db\n",
    "\n",
    "The original data was collected through Spotify’s Web API, which provides track level audio analysis values and popularity scores.\n",
    "\n",
    "## Variable Descriptions\n",
    "Below is a list of the key variables we will be using:\n",
    "- genre: Genre label associated with the track.\n",
    "- artist_name: Name of the performing artist.\n",
    "- track_name: Name of the song.\n",
    "- track_id: Unique Spotify ID for the track.\n",
    "- popularity: Spotify-defined popularity score (0–100).\n",
    "- acousticness: Confidence measure of whether a track is acoustic (0.0–1.0).\n",
    "- danceability: How suitable a track is for dancing (0.0–1.0).\n",
    "- duration_ms: Track duration in milliseconds.\n",
    "- energy: Perceived intensity and activity of the track (0.0–1.0).\n",
    "- instrumentalness: Likelihood the track contains no vocals (0.0–1.0).\n",
    "- key: Musical key (0–11).\n",
    "- liveness: Probability the track was performed live (0.0–1.0).\n",
    "- loudness: Overall loudness of the track (in decibels).\n",
    "- mode: Modal quality of the track (0 = minor, 1 = major).\n",
    "- speechiness: Presence of spoken words in a track (0.0–1.0).\n",
    "- tempo: Estimated tempo in beats per minute (BPM).\n",
    "- time_signature: Estimated time signature (e.g., 3, 4, 5).\n",
    "- valence: Musical “positiveness” conveyed by the track (0.0–1.0).\n",
    "\n",
    "## Spotify Popularity Score\n",
    "Spotify’s popularity metric ranges from 0 to 100, where higher values indicate a track that is streamed more often and more recently relative to others. The score is not based on audio features; instead, it is derived from listener behavior such as stream counts, recency, playlist appearances, and user engagement. Because it reflects real-world streaming activity, this popularity score serves as a meaningful target variable for both regression and classification tasks.\n",
    "\n",
    "## Load Spotify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc37b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/SpotifyFeatures.csv')\n",
    "\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b4504",
   "metadata": {},
   "source": [
    "### Dataset Summary\n",
    "\n",
    "The dataset contains **232,725 rows** and **18 columns**. Most of the variables are numeric \n",
    "features such as danceability, energy, valence, loudness, tempo, and other audio related \n",
    "attributes generated by Spotify's API. A few variables, such as `track_name`, `artist_name`, \n",
    "and `genre`, are stored as object/string types.\n",
    "\n",
    "Numeric columns include float based audio features (danceability, energy, valence, acousticness, \n",
    "speechiness, instrumentalness, etc) as well as integer based metadata (popularity, key, \n",
    "time_signature). The object columns contain track metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ce8ea",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning & Preparation\n",
    "\n",
    "This section ensures the dataset is complete, consistent, and ready for modeling. We check for missing values, remove duplicates, engineer new features, and prepare  \n",
    "the final feature set used in regression and classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c90acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b26605",
   "metadata": {},
   "source": [
    "### Missing Values Summary\n",
    "\n",
    "The missing values check shows that the dataset is almost completely full, with only \n",
    "one missing value found in the `track_name` column. Since every other column has zero \n",
    "missing entries and the dataset contains over 232,000 rows, removing this single row \n",
    "is the simplest and most appropriate approach. This ensures data consistency without \n",
    "any meaningful loss of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fceeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb074e0",
   "metadata": {},
   "source": [
    "After removing the single row containing a missing value, the dataset remains large \n",
    "and complete. This prepares the data for further preprocessing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97358642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for and remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8062b230",
   "metadata": {},
   "source": [
    "### Duplicate Check\n",
    "\n",
    "We checked for duplicate rows using `df.drop_duplicates()`. The shape of the dataset \n",
    "remained the same before and after this operation, indicating that there were no \n",
    "duplicate tracks present. This means the dataset is already clean with respect to \n",
    "duplicated entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe22c2",
   "metadata": {},
   "source": [
    "## Creating New Features\n",
    "\n",
    "### 1. Converting duration from milliseconds to minutes:\n",
    "We convert track duration from milliseconds to minutes for better interpretability \n",
    "in visualizations and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration_min'] = df['duration_ms'] / 60000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c816f5",
   "metadata": {},
   "source": [
    "### 2. Create “high_popularity” variable:\n",
    "To create a balanced classification target, we define a track as \"high popularity\" if \n",
    "its popularity score is above the median of the dataset. This ensures an approximately \n",
    "50/50 split between high and low popularity classes. We use the median threshold, which is best for balanced classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a9341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = df['popularity'].median()\n",
    "df['high_popularity'] = (df['popularity'] >= threshold).astype(int)\n",
    "threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c4581f",
   "metadata": {},
   "source": [
    "## Selecting Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127046c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'danceability', 'energy', 'valence', 'loudness', 'tempo',\n",
    "    'acousticness', 'instrumentalness', 'speechiness', 'liveness',\n",
    "    'duration_min'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y_reg = df['popularity']\n",
    "y_clf = df['high_popularity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb304e9",
   "metadata": {},
   "source": [
    "We define our feature matrix `X` using key audio characteristics that are numeric \n",
    "and suitable for regression and classification. Our regression target is the continuous \n",
    "popularity score, and our classification target is the binary high_popularity variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63effac",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Before building predictive models, in this section we explore the structure of the dataset and examine how Spotify audio features relate to popularity. This includes:\n",
    "\n",
    "- distribution plots  \n",
    "- correlation heatmaps  \n",
    "- scatterplots  \n",
    "- high vs low popularity comparisons  \n",
    "- genre-level differences  \n",
    "\n",
    "These exploratory analyses will help us anticipate model performance and understand the relationships among the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61865eff",
   "metadata": {},
   "source": [
    "## Correlation Heatmap\n",
    "The correlation heatmap helps identify which audio features are most closely \n",
    "associated with popularity. For example, features like loudness, energy, or \n",
    "danceability may show moderate positive correlation with popularity, while others \n",
    "such as acousticness may show weaker or negative associations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf226bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df[features + ['popularity']].corr(), cmap='viridis', annot=False)\n",
    "plt.title('Correlation Heatmap of Audio Features and Popularity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772761c",
   "metadata": {},
   "source": [
    "### Interpretation of Correlation Heatmap\n",
    "\n",
    "The correlation heatmap provides an overview of how Spotify’s audio features relate both to one another and to a track’s popularity score. The most important takeaway is that popularity has only weak linear relationships with any individual audio feature. Features such as loudness, energy, danceability, and valence show small positive correlations with popularity, but the magnitudes are low (roughly 0.15–0.22), indicating that these characteristics only modestly influence how popular a song becomes. Likewise, acousticness shows a moderate negative correlation with popularity, meaning that highly acoustic tracks tend to be less popular on average. Overall, the heatmap suggests that popularity is not strongly determined by musical attributes alone, which aligns with Spotify’s metric being heavily driven by user behavior, recency, and playlist exposure rather than audio characteristics.\n",
    "\n",
    "A more noticeable pattern appears in the relationships among the audio features themselves. Certain features—particularly energy and loudness—are very strongly correlated, reflecting that they capture similar underlying musical intensity. Acousticness also shows a strong negative correlation with both energy and loudness, consistent with the contrast between quiet, acoustic tracks and louder, high-intensity songs. Meanwhile, other features such as tempo, liveness, speechiness, and duration show almost no correlation with popularity and only weak relationships with other features. These internal feature relationships have important implications for modeling, because clusters of highly correlated predictors can introduce multicollinearity, making it harder for linear regression to isolate the effect of any single feature.\n",
    "\n",
    "In the context of our research questions, this heatmap offers early evidence for several conclusions.\n",
    "For RQ1, the correlations indicate that although some features (like loudness or energy) relate to popularity, none serve as strong predictors, which foreshadows the modest performance we later observe in regression and classification models.\n",
    "For RQ4, the heatmap directly answers the question about relationships among audio features by revealing meaningful structure—such as the strong loudness–energy connection and the acousticness–energy contrast.\n",
    "Finally, the weak correlations between predictors and popularity help explain the results in RQ2 and RQ3, where both logistic and linear models struggle to achieve high predictive accuracy.\n",
    "\n",
    "Overall, the correlation heatmap establishes an important foundation for the rest of the analysis: popularity is only weakly tied to musical attributes, and audio features themselves are often highly interrelated, both of which shape how effective our statistical models can be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad799f",
   "metadata": {},
   "source": [
    "## Popularity Distribution\n",
    "Popularity scores range from 0–100, with many tracks clustered at lower values. \n",
    "This indicates that most songs on Spotify are relatively less popular, and only \n",
    "a smaller subset achieves high popularity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['popularity'], kde=True, bins=30)\n",
    "plt.title(\"Distribution of Popularity Scores\")\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f2d1d",
   "metadata": {},
   "source": [
    "### Interpretation of Popularity Distribution\n",
    "\n",
    "The distribution of Spotify popularity scores reveals that popularity is not evenly spread across songs, and instead follows a moderately right-skewed pattern. The histogram shows a large concentration of tracks with popularity values between roughly 20 and 60, with the highest density near the 40–55 range, indicating that most tracks in the dataset achieve only moderate recognition on the platform. At the same time, there is a clear long tail stretching toward higher popularity values, showing that only a small fraction of songs reach very high popularity (above 70 or 80), and extremely popular tracks (90+) are rare. The left side of the distribution also contains a noticeable spike near zero, reflecting the large number of tracks that have almost no recent streams or listener engagement. Taken together, this pattern suggests that Spotify’s catalog is dominated by mid-tier and low-engagement songs, with only a select minority achieving widespread success.\n",
    "\n",
    "This distribution has important implications for our analysis and directly supports several of our research questions. For RQ2 (classification), the shape of the distribution justifies using the median popularity score as the threshold for defining “high” vs “low” popularity, because the distribution is not symmetric and we want balanced classes. For RQ3 (regression modeling), the spread of the histogram visually explains why predicting an exact popularity score is challenging—the wide dispersion and heavy right tail indicate substantial inherent variability that audio features alone may not capture. Additionally, the skewness suggests that regression residuals may not follow perfect normality, which is important to acknowledge when assessing model assumptions.\n",
    "\n",
    "More broadly, this distribution reinforces a key theme in our project: popularity behaves like a behavioral metric, not a purely musical one. The uneven, right-skewed shape reflects the influence of external factors such as playlisting, fanbase size, marketing, and release timing. Understanding this distribution sets realistic expectations for the predictive power of our models and provides important context for interpreting both linear regression and logistic regression results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42fc290",
   "metadata": {},
   "source": [
    "## Audio Feature Distribution\n",
    "These distributions show how Spotify's audio attributes vary across tracks. Some \n",
    "features (such as danceability and energy) follow more uniform distributions, \n",
    "while others (like acousticness or instrumentalness) show heavy skewness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9425639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = ['danceability', 'energy', 'valence', 'acousticness', 'loudness']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(cols_to_plot, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f813f",
   "metadata": {},
   "source": [
    "### Interpretation of Feature Distributions\n",
    "\n",
    "The distribution plots for the audio features highlight just how diverse and unevenly structured Spotify tracks are across their musical characteristics. Several features—such as danceability and energy—display shapes that resemble smoothed unimodal distributions, with most songs clustering around moderate values. Danceability peaks around 0.6, suggesting that Spotify’s library is dominated by rhythmically “mid-tempo” tracks rather than extremely low- or high-danceability songs. Energy shows a flatter but still right-shifted distribution, with many songs falling between 0.5 and 0.8, reflecting the prevalence of moderately energetic to upbeat tracks across genres.\n",
    "\n",
    "In contrast, features like acousticness, instrumentalness, and speechiness show extreme right or left skew, with very large spikes near 0. These distributions indicate that most songs on Spotify are not acoustic, not instrumental, and contain very little spoken content—properties consistent with the dominance of mainstream vocal, studio-produced music on the platform. The large mass at zero for acousticness and instrumentalness is especially meaningful for modeling: these features do not vary continuously across most tracks, which limits their predictive contribution in linear or logistic regression. Such skewed variables may have nonlinear relationships with popularity or interact with genre rather than acting independently.\n",
    "\n",
    "The loudness distribution is the most distinctly shaped, showing a strong peak around −7 to −12 dB. This cluster reflects the “loudness normalization” practices common in modern audio production, where tracks are mastered to similar loudness levels. The long left tail toward −50 dB represents rare cases such as ambient, classical, or spoken-word recordings. Because loudness has a more regular, bell-shaped distribution, it is one of the features most amenable to linear modeling—which aligns with our later finding that loudness emerges as one of the strongest positive predictors of popularity.\n",
    "\n",
    "Overall, these feature distributions are essential to understanding RQ1 (Which features affect popularity?) and RQ4 (How are audio features related to one another?), because they reveal that the dataset contains a mix of well-behaved variables and highly skewed variables. This explains later modeling results: features with limited variation or extreme skew tend not to contribute meaningful predictive power, while more continuous, regularly distributed features (like loudness, danceability, and energy) play a larger role. These patterns also help set expectations for the performance of regression and classification models, since skewed or zero-inflated variables make it difficult to uncover strong linear effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3acec7",
   "metadata": {},
   "source": [
    "## Audio Features vs. Popularity\n",
    "Scatterplots allow us to visually check whether relationships appear linear or \n",
    "nonlinear. Some features may show mild positive relationships with popularity, \n",
    "while others show little structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33cbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x='danceability', y='popularity', data=df, alpha=0.3)\n",
    "plt.title(\"Danceability vs Popularity\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='energy', y='popularity', data=df, alpha=0.3)\n",
    "plt.title(\"Energy vs Popularity\")\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='valence', y='popularity', data=df, alpha=0.3)\n",
    "plt.title(\"Valence vs Popularity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e931af",
   "metadata": {},
   "source": [
    "### Interpretation of Scatterplots\n",
    "\n",
    "These scatterplots illustrate how individual audio features (danceability, energy, and valence) relate to a song’s popularity score. Across all three plots, the same pattern emerges: while there are slight positive trends, the relationships are extremely weak and dominated by heavy dispersion. This confirms that no single audio feature reliably predicts popularity, and that popularity appears to be influenced by many external factors beyond the audio profile of the track.\n",
    "\n",
    "In the Danceability vs. Popularity scatterplot, there is a mild upward slope, suggesting that more danceable songs tend to score slightly higher on average. However, the spread at every danceability level is enormous—songs with the same danceability value can have popularity scores anywhere from 0 to 100. This heavy vertical scatter indicates very low explanatory power, reinforcing the small correlation seen in the heatmap (≈0.22).\n",
    "\n",
    "The Energy vs. Popularity plot shows a nearly identical pattern. Popularity rises slightly as energy increases, but again, songs with high energy exist across the entire popularity spectrum. This means that while high-energy songs may be slightly more likely to perform well, energy alone is far from being a sufficient or reliable predictor.\n",
    "\n",
    "The Valence vs. Popularity scatterplot shows perhaps the weakest pattern of all. Songs with positive or negative emotional tone appear widely spread across popularity values, demonstrating no meaningful linear or nonlinear trend. Valence essentially provides no predictive value for popularity and confirms its extremely low correlation in the heatmap.\n",
    "\n",
    "Overall, these scatterplots visually support our earlier statistical findings:\n",
    "- The audio features do not provide strong predictive power.\n",
    "- Any trends that do exist are small and overshadowed by massive variability.\n",
    "- This makes high-accuracy regression or classification inherently difficult.\n",
    "\n",
    "In the context of our research questions—particularly RQ1 (“Which audio features predict popularity?”) and RQ3 (“How well can we predict popularity using regression?”)—these plots provide strong evidence that audio content alone is insufficient for accurate prediction, and that external behavioral factors (playlisting, marketing, artist reputation) likely dominate Spotify’s popularity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c662f67",
   "metadata": {},
   "source": [
    "## High vs. Low Popularity Comparisons\n",
    "These boxplots compare the distribution of audio features between high and low \n",
    "popularity groups. Features like loudness and energy often show clear differences, \n",
    "suggesting they may play a role in predicting popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b6097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='high_popularity', y='danceability', data=df)\n",
    "plt.title(\"Danceability: High vs Low Popularity\")\n",
    "plt.xlabel(\"High Popularity (0 = No, 1 = Yes)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='high_popularity', y='energy', data=df)\n",
    "plt.title(\"Energy: High vs Low Popularity\")\n",
    "plt.xlabel(\"High Popularity (0 = No, 1 = Yes)\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='high_popularity', y='loudness', data=df)\n",
    "plt.title(\"Loudness: High vs Low Popularity\")\n",
    "plt.xlabel(\"High Popularity (0 = No, 1 = Yes)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0e6b9",
   "metadata": {},
   "source": [
    "### Interpretation of High vs Low Popularity Comparisons\n",
    "\n",
    "These boxplots compare danceability, energy, and loudness between low-popularity and high-popularity songs. Overall, they show consistent but modest differences in musical characteristics — useful for understanding trends, but not strong enough to yield highly accurate predictions.\n",
    "\n",
    "Danceability:\n",
    "High-popularity tracks tend to be slightly more danceable, with a higher median and an upward shift in the overall distribution. This suggests that more mainstream songs often follow rhythmic and dance-oriented structures, but the large overlap between groups indicates that danceability alone is not a strong separator.\n",
    "\n",
    "Energy:\n",
    "Popular songs generally show higher energy levels. The median is higher for the high-popularity group, and the distribution is shifted upward. This aligns with the idea that listeners favor songs with a stronger beat and intensity. However, as with danceability, the overlap between groups reflects only a weak relationship.\n",
    "\n",
    "Loudness:\n",
    "Loudness shows the clearest separation. High-popularity songs are mastered louder on average, consistent with commercial production standards. The median difference is noticeable and statistically meaningful (supported by the bootstrap CI). Even so, the overlap remains large, meaning loudness is helpful but still not a strong standalone predictor.\n",
    "\n",
    "Overall:\n",
    "Across all three features, high-popularity songs tend to be more danceable, more energetic, and louder, but the differences are small. These patterns help explain mild trends seen in the correlation heatmap and scatterplots, and they reinforce our modeling results:\n",
    "\n",
    "Audio features capture some aspects of popular music, but they are not strong enough to reliably distinguish high vs. low popularity on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec9e20",
   "metadata": {},
   "source": [
    "## Genre-Level Popularity\n",
    "Some genres tend to have higher average popularity than others. This analysis helps \n",
    "identify genre-level trends and supports further questions about how musical style \n",
    "relates to popularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_pop = df.groupby('genre')['popularity'].mean().sort_values(ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=genre_pop.values, y=genre_pop.index)\n",
    "plt.title(\"Top 15 Genres by Average Popularity\")\n",
    "plt.xlabel(\"Average Popularity\")\n",
    "plt.ylabel(\"Genre\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe18e4",
   "metadata": {},
   "source": [
    "### Interpretation of Genre-Level Popularity\n",
    "\n",
    "This bar chart shows the average Spotify popularity score for the 15 highest-performing genres in the dataset. The results highlight clear differences in how musical style relates to listener engagement.\n",
    "\n",
    "Pop, Rap, Rock, and Hip-Hop dominate the top of the ranking, each averaging well above most other genres. These genres are heavily represented in mainstream playlists, radio rotations, and social media trends, which likely contributes to their higher popularity. Their widespread commercial appeal means they reach larger audiences, receiving more streams and playlist placements — factors that Spotify’s popularity metric directly captures.\n",
    "\n",
    "Genres such as Dance, Indie, Children's Music, and R&B also perform fairly well, reflecting strong engagement within specific listener communities. Their moderate-to-high scores suggest a combination of broad appeal and niche listener loyalty.\n",
    "\n",
    "Lower in the top 15, genres like Folk, Soul, Country, Jazz, Electronic, and Reggaeton show noticeably lower average popularity. This does not imply lower artistic quality but reflects smaller listener bases or less frequent playlist promotion. Many of these genres cater to more specific audiences and may not receive the same level of algorithmic exposure.\n",
    "\n",
    "Overall, the chart demonstrates that popularity is strongly tied to genre-level trends, supporting our research question on genre influence (RQ5). These differences also reinforce a key conclusion of the project:\n",
    "\n",
    "Popularity cannot be explained by audio features alone — genre, marketing, exposure, and cultural relevance play major roles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84999f4",
   "metadata": {},
   "source": [
    "# 5. Statistical Modeling\n",
    "\n",
    "We now evaluate how well audio features can predict Spotify track popularity through:  \n",
    "- linear regression (predicting continuous popularity)  \n",
    "- logistic regression (predicting high vs low popularity)  \n",
    "- cross-validation (model stability)  \n",
    "- bootstrap inference (uncertainty estimation)  \n",
    "\n",
    "Each model directly supports our research questions and helps determine the extent to  \n",
    "which musical characteristics influence popularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb9f3b-070e-4abf-9b2a-0bccd91e6d27",
   "metadata": {},
   "source": [
    "### Running Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772fe79-301d-42a0-869f-a922d32d6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix + intercept\n",
    "Xmat = np.column_stack([np.ones(len(X)), X.values])\n",
    "yvec = y_reg.values\n",
    "\n",
    "# Compute coefficients (OLS closed form)\n",
    "beta = np.linalg.inv(Xmat.T @ Xmat) @ (Xmat.T @ yvec)\n",
    "\n",
    "y_pred = Xmat @ beta\n",
    "\n",
    "rmse = np.sqrt(np.mean((yvec - y_pred)**2))\n",
    "sst = np.sum((yvec - yvec.mean())**2)\n",
    "sse = np.sum((yvec - y_pred)**2)\n",
    "r2 = 1 - sse/sst\n",
    "\n",
    "print(f'RMSE = {rmse:.04f}, r2 = {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual Plot for Linear Regression\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(yvec, y_pred, alpha=0.2)\n",
    "plt.plot([0,100], [0,100], color='red', linewidth=2)\n",
    "plt.xlabel(\"Actual Popularity\")\n",
    "plt.ylabel(\"Predicted Popularity\")\n",
    "plt.title(\"Predicted vs Actual Popularity (Linear Regression)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ad002",
   "metadata": {},
   "source": [
    "### Linear Regression Plot Interpretation\n",
    "This plot compares the regression model's predicted popularity scores to the actual scores. The strong clustering around the mid-range shows that the model predicts values close to the average for most tracks. Predictions rarely reach very low or very high values, and there is large vertical spread at every x-value.\n",
    "\n",
    "This confirms that the regression model cannot accurately predict exact popularity, consistent with the low R² (0.23). Audio features provide limited predictive power, and external factors likely dominate popularity outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c90361",
   "metadata": {},
   "source": [
    "### 5.1 Linear Regression Results\n",
    "\n",
    "The linear regression model produced an RMSE of approximately **16** and R² of **0.23**,  \n",
    "meaning audio features explain only 23% of the variability in popularity.\n",
    "\n",
    "This indicates:\n",
    "- popularity is influenced more by external factors than audio properties  \n",
    "- no combination of these features can accurately predict exact popularity  \n",
    "- feature effects are real but small  \n",
    "\n",
    "Danceability and loudness were the strongest positive predictors, while acousticness and valence had negative associations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d013a4-ca4a-4965-bce8-cdc916d1b29d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Logistic Regression (Gradient Descent Implementation)\n",
    "We classify high vs low popularity using a hand-coded logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d1275-65e9-4356-b997-be7c9717bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Prepare matrices\n",
    "Xcl = np.column_stack([np.ones(len(X)), X.values])  # intercept\n",
    "ycl = y_clf.values.reshape(-1,1)\n",
    "\n",
    "# Initialize parameters\n",
    "np.random.seed(0)\n",
    "beta_cl = np.zeros((Xcl.shape[1], 1))\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 5000\n",
    "\n",
    "for i in range(epochs):\n",
    "    z = Xcl @ beta_cl\n",
    "    p = sigmoid(z)\n",
    "    grad = Xcl.T @ (p - ycl)\n",
    "    beta_cl -= lr * grad / len(Xcl)\n",
    "\n",
    "beta_cl.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822630f0-064c-463d-b2bc-8862efad91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = sigmoid(Xcl @ beta_cl).flatten()\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "accuracy = (preds == y_clf.values).mean()\n",
    "print(f'Accuracy = {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726f430",
   "metadata": {},
   "source": [
    "### 5.2 Logistic Regression Results (High vs Low Popularity)\n",
    "\n",
    "The logistic regression classifier achieved an accuracy of **~49%**, which is near random guessing. This shows that audio features do not reliably separate high‐popularity songs from low‐popularity songs.\n",
    "\n",
    "The weak separation confirms our earlier findings that musical characteristics contribute only modestly to popularity outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d9cae-d117-4c0a-8236-f0f54ae9045f",
   "metadata": {},
   "source": [
    "\n",
    "### Manual K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625d431-e8c0-4bc3-83ec-1d3cdb95900c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols(X_train, y_train):\n",
    "    return np.linalg.inv(X_train.T @ X_train) @ (X_train.T @ y_train)\n",
    "\n",
    "def rmse_func(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "K = 5\n",
    "fold_size = len(X) // K\n",
    "rmse_scores = []\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "for k in range(K):\n",
    "    test_idx = indices[k*fold_size:(k+1)*fold_size]\n",
    "    train_idx = np.delete(indices, test_idx)\n",
    "\n",
    "    X_train = np.column_stack([np.ones(len(train_idx)), X.iloc[train_idx].values])\n",
    "    y_train = y_reg.iloc[train_idx].values\n",
    "\n",
    "    X_test = np.column_stack([np.ones(len(test_idx)), X.iloc[test_idx].values])\n",
    "    y_test = y_reg.iloc[test_idx].values\n",
    "\n",
    "    beta_k = ols(X_train, y_train)\n",
    "    preds_k = X_test @ beta_k\n",
    "\n",
    "    rmse_scores.append(rmse_func(y_test, preds_k))\n",
    "\n",
    "rmse_cv_mean = np.mean(rmse_scores)\n",
    "rmse_cv_std = np.std(rmse_scores)\n",
    "\n",
    "print(f'RMSE = {rmse_cv_mean:.4f}, std. dev. = {rmse_cv_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce0dca6-a578-4fce-ada5-2896628d1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic(X_train, y_train, lr=0.001, epochs=2000):\n",
    "    beta = np.zeros((X_train.shape[1], 1))\n",
    "    for i in range(epochs):\n",
    "        p = sigmoid(X_train @ beta)\n",
    "        grad = X_train.T @ (p - y_train)\n",
    "        beta -= lr * grad / len(X_train)\n",
    "    return beta\n",
    "\n",
    "acc_scores = []\n",
    "\n",
    "for k in range(K):\n",
    "    test_idx = indices[k*fold_size:(k+1)*fold_size]\n",
    "    train_idx = np.delete(indices, test_idx)\n",
    "\n",
    "    X_train = np.column_stack([np.ones(len(train_idx)), X.iloc[train_idx].values])\n",
    "    y_train = y_clf.iloc[train_idx].values.reshape(-1,1)\n",
    "\n",
    "    X_test = np.column_stack([np.ones(len(test_idx)), X.iloc[test_idx].values])\n",
    "    y_test = y_clf.iloc[test_idx].values\n",
    "\n",
    "    beta_k = fit_logistic(X_train, y_train)\n",
    "    \n",
    "    probs = sigmoid(X_test @ beta_k).flatten()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc_scores.append((preds == y_test).mean())\n",
    "\n",
    "print(f'Accuracy avg. = {np.mean(acc_scores):.4f}, std. dev. = {np.std(acc_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f03ad8",
   "metadata": {},
   "source": [
    "### 5.3 Cross-Validation Results\n",
    "\n",
    "Cross-validation gave an RMSE of **~16.5 ± 1.9** for regression and classification accuracy of **~56% ± 11%**. These results indicate:\n",
    "\n",
    "- regression performance is stable but not highly accurate  \n",
    "- classification performance is unstable across folds  \n",
    "- the weak predictive power is consistent across multiple splits  \n",
    "\n",
    "This further confirms that audio features alone have limited explanatory power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bf328-70e4-4458-95a5-b09a2be7ef79",
   "metadata": {},
   "source": [
    "\n",
    "### Bootstrap Confidence Intervals\n",
    "##### Bootstrap CI for Mean Loudness Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8622b7f-c93d-428f-b65c-6de423912754",
   "metadata": {},
   "outputs": [],
   "source": [
    "high = df[df['high_popularity']==1]['loudness'].values\n",
    "low  = df[df['high_popularity']==0]['loudness'].values\n",
    "\n",
    "obs = high.mean() - low.mean()\n",
    "\n",
    "B = 2000\n",
    "boot = []\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for _ in range(B):\n",
    "    hb = rng.choice(high, len(high), replace=True)\n",
    "    lb = rng.choice(low,  len(low),  replace=True)\n",
    "    boot.append(hb.mean() - lb.mean())\n",
    "\n",
    "ci_low, ci_high = np.percentile(boot, [2.5, 97.5])\n",
    "\n",
    "print(f'Mean difference = {obs:.4f}, Confidence interval = {ci_low, ci_high}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca89e36-4dc8-41eb-889a-07265236006a",
   "metadata": {},
   "source": [
    "\n",
    "##### Bootstrap CI for Logistic Regression Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf5b11-5bb8-44b9-ac8b-e7ff22b771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_full = sigmoid(Xcl @ beta_cl).flatten()\n",
    "preds_full = (probs_full >= 0.5).astype(int)\n",
    "truth = y_clf.values\n",
    "\n",
    "B = 2000\n",
    "boot_acc = []\n",
    "\n",
    "for _ in range(B):\n",
    "    idx = rng.integers(0, len(truth), len(truth))\n",
    "    boot_acc.append((preds_full[idx] == truth[idx]).mean())\n",
    "\n",
    "np.percentile(boot_acc, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9964d0f",
   "metadata": {},
   "source": [
    "### 5.4 Bootstrap Confidence Intervals\n",
    "\n",
    "Bootstrap analysis confirmed two key findings:\n",
    "\n",
    "1. **Loudness difference between high and low popularity**  \n",
    "   CI ≈ (3.82, 3.91) dB  \n",
    "   → popular songs are consistently louder\n",
    "\n",
    "2. **Logistic regression accuracy**  \n",
    "   CI ≈ (48.76%, 49.16%)  \n",
    "   → classifier performs no better than random guessing\n",
    "\n",
    "These results strongly reinforce our earlier conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(boot, bins=40, color='skyblue', edgecolor='black')\n",
    "plt.axvline(ci_low, color='red', linestyle='--', linewidth=2)\n",
    "plt.axvline(ci_high, color='red', linestyle='--', linewidth=2)\n",
    "plt.title(\"Bootstrap Distribution of Loudness Difference\\n(High Popularity - Low Popularity)\")\n",
    "plt.xlabel(\"Mean Difference in Loudness (dB)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0c8116",
   "metadata": {},
   "source": [
    "### Bootstrap Distribution Interpretation\n",
    "This plot shows the bootstrap distribution of the mean loudness difference between high-popularity and low-popularity songs. The distribution is tightly centered around ~3.85 dB, and the 95% confidence interval (shown by the dashed red lines) is narrow and does not cross zero.\n",
    "\n",
    "This confirms that high-popularity songs are consistently louder than low-popularity songs, and this difference is statistically stable. This supports our earlier findings and reinforces that loudness has a small but reliable relationship with popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cf2b4-e21f-4f47-ab08-09daa029ad63",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f331b2-2de9-4929-9ec6-b3a137544879",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": [\"intercept\"] + features,\n",
    "    \"coef\": beta\n",
    "})\n",
    "\n",
    "print(coef_df.sort_values(\"coef\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_no_intercept = coef_df[coef_df['feature'] != 'intercept']\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "coef_no_intercept.set_index('feature')['coef'].sort_values().plot(kind='barh')\n",
    "plt.title(\"Feature Importance (Linear Regression Coefficients)\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fd64a8",
   "metadata": {},
   "source": [
    "### Feature Importance Interpretation\n",
    "This bar plot shows the linear regression coefficients for each audio feature. Danceability and loudness have the largest positive coefficients, meaning more danceable and louder tracks tend to receive slightly higher popularity scores. Features such as valence, acousticness, and liveness have negative coefficients, indicating small downward associations.\n",
    "\n",
    "However, all coefficients are relatively small, which matches our low R² value. These features contribute some signal, but none are strong predictors of popularity, reinforcing RQ1 and RQ3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fc3cc",
   "metadata": {},
   "source": [
    "### Feature Importance Summary\n",
    "\n",
    "Danceability and loudness are the strongest positive predictors of popularity. Acousticness, valence, speechiness, and liveness are negatively associated.\n",
    "\n",
    "However, all coefficient magnitudes are relatively small, which aligns with the limited predictive accuracy of our models. These features describe sonic character, but popularity depends on many outside factors beyond audio alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b222f5",
   "metadata": {},
   "source": [
    "# 6. Interpretation & Reporting\n",
    "\n",
    "This project examined how well Spotify’s audio features—danceability, energy, loudness, valence, acousticness, and others—can explain or predict a track’s popularity. Across our exploratory analysis, regression modeling, classification modeling, cross-validation, and bootstrap uncertainty estimation, a consistent conclusion emerged: audio features matter, but only to a limited extent.\n",
    "\n",
    "**Key Findings**\n",
    "\n",
    "Popular songs tend to be louder, more energetic, and more danceable, and they generally have lower acousticness. These trends appeared consistently in the correlation heatmap, boxplots, and our regression coefficients.\n",
    "\n",
    "Genre plays a noticeable role: Pop, Rap, Hip-Hop, and Rock exhibit much higher average popularity than most other genres. This suggests that popularity is strongly shaped by cultural and industry dynamics, not just audio properties.\n",
    "\n",
    "Despite these patterns, audio features explain only about 23% of the variability in popularity (R² ≈ 0.23). This means that over three-quarters of the variation in Spotify popularity is unrelated to the sonic attributes captured in this dataset.\n",
    "\n",
    "Our logistic regression classifier performed only at or slightly above random accuracy, even with cross-validation. This shows that no combination of audio features can reliably separate “high popularity” from “low popularity” tracks.\n",
    "\n",
    "Bootstrap analyses confirmed that while some effects—like loudness differences—are statistically consistent, the predictive models themselves remain weak and unstable.\n",
    "\n",
    "**Overall Interpretation**\n",
    "\n",
    "Taken together, these results highlight a fundamental characteristic of Spotify’s popularity metric:\n",
    "Popularity behaves much more like a behavioral variable than a musical one.\n",
    "\n",
    "User listening behavior, playlist placement, algorithmic exposure, artist fame, release timing, and promotional strategies likely dominate what makes a song popular. Audio features capture the sound of a track, but not the context in which it is discovered, promoted, or shared. This explains the large residual variability in our regression model and the low classification performance.\n",
    "\n",
    "**Implications**\n",
    "\n",
    "Our findings imply that understanding popularity requires additional data sources beyond audio features alone. Predictive accuracy would likely improve by incorporating variables such as:\n",
    "- number of playlist placements\n",
    "- artist monthly listeners\n",
    "- release year and recency\n",
    "- social media trends\n",
    "- marketing activity or label backing\n",
    "\n",
    "Audio-based models may be better suited for describing stylistic differences or identifying sonic trends, rather than predicting real-world popularity.\n",
    "\n",
    "**Final Conclusion**\n",
    "\n",
    "Although audio features provide some insight into what popular songs sound like, they have limited predictive power for actual Spotify popularity. This reflects the complex ecosystem in which music gains traction. Popularity is shaped by audience behavior, industry influence, and cultural momentum—factors far beyond the scope of audio alone.\n",
    "\n",
    "Our analysis ultimately reinforces a key theme: music popularity cannot be fully explained by the sound of the music itself, and any realistic predictive system must integrate both musical and non-musical information.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
